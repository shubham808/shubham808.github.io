---
layout: post
title: Week 1
---

The first week of coding period ends today and we have an idea about how we are going to be testing the premature stopping Framework. What we have in mind is using a
registered callback along with an addcallback method that allows the user to define a custom callback to an algorithm. As toni told me what we want to do ideally is
train a model, stop and serialize it, and then calculate the result. Next, we compare it with the result we get by deserializing the model we saved earlier for consistency.

Obviously, doing this for all iterative algorithms seperately is difficult so we are going to be writing some TYPED_TESTS. These can run for a large instances
of algorithms without us having to explicitly write them for each instance. This is intuitive because we want to test just the fact that the model is serialized consistently
and also the callback will remain same for every instance. Toni has made some edits to the CStoppableSGObject class regarding this.

I will be working on this the following week.

We also ran into an issue because the LinearMachine and KernelMachine were implementing their own version of train instead of using the base class version.
Due to this we were not able to write custom on_pause_impl() methods. So I have made a patch to that.

We wrote code for the StoppableSGObject class last week and I implemented it in CMachineEvaluation.

Next we took a look at removing the direct calls to cancel_computation() and replacing them with CANCEL_COMPUTATION macro. During this we ran into an issue with using
the macro with const train methods.

The log-det refactoring was finally completed this week. Cheers to that! The final thing we did was making the code more memory efficient while preserving thread-safety.
We used a boolean flag that defaults to false. We will set it to true when we negated shifts and just let it be false otherwise. This was a simple hack and provides a lot of
memory efficiency due to the fact that earlier we were just allocating a vector memory and then allocating another all over again in the next iteration.

The final thing i did this week was trying to come up with a list of iterative algorithms. This meant going through a lot of code and finding classes with some
logical iterative implementation. First I found a list of all algorithms with CMachine as base class. From there I manually go through all the for/while loops and
decide if this will need to be visited later in this project along with the line number that the loop starts at. The list is not complete yet but we have enough to start our work.
This was added as a Wiki page to shogun. I tried to keep the list short by writing only base classes like say NeuralNetworks/EMBase, we will need to visit all others that use it when
we start implementing custom pause/cancel behaviours.


### Pull Requests:
---
[#4286 using CANCEL_COMPUTATION macro](https://github.com/shogun-toolbox/shogun/pull/4286)<br>
replacing cancel_computation() with the macro.  

[#4291 refactor CMachineEvaluation](https://github.com/shogun-toolbox/shogun/pull/4291)<br>
removing duplicate code from the class and inheriting from CStoppableSGObject.

[#4235 parallel computation of log-det](https://github.com/shogun-toolbox/shogun/pull/4235)<br>
making all methods called within estimator.sample const along with parallelizing in it.

[#4287 connect LinearMachine and KernelMachine to signal Handler](https://github.com/shogun-toolbox/shogun/pull/4287)<br>
use the train of base class in them.

[List of iterative Algorithms](https://github.com/shogun-toolbox/shogun/wiki/List-of-iterative-algorithms)<br>
